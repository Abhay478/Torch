{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module): \n",
    "    # def __init__(self) -> None:\n",
    "    #     super(MyCNN, self).__init__()\n",
    "    #     self.stack = nn.Sequential(\n",
    "    #         nn.Conv2d(3, 6, 5),\n",
    "    #         nn.MaxPool2d(2, 2),\n",
    "    #         nn.Conv2d(6, 16, 5),\n",
    "    #         nn.MaxPool2d(2, 2),\n",
    "    #         nn.Flatten(),\n",
    "    #         nn.Linear(400, 128),\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Linear(128, 32),\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Linear(32, 10)\n",
    "    #     )\n",
    "        \n",
    "    # def forward(self, x):\n",
    "    #     return self.stack(x)\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 150)\n",
    "        self.fc2 = nn.Linear(150, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "model = MyCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool2d(\n",
      "  kernel_size=tensor([[[0.4040]],\n",
      "  \n",
      "          [[0.1349]],\n",
      "  \n",
      "          [[0.0000]],\n",
      "  \n",
      "          [[0.2061]],\n",
      "  \n",
      "          [[0.1280]],\n",
      "  \n",
      "          [[0.0000]]], grad_fn=<ReluBackward0>), stride=tensor([[[0.4040]],\n",
      "  \n",
      "          [[0.1349]],\n",
      "  \n",
      "          [[0.0000]],\n",
      "  \n",
      "          [[0.2061]],\n",
      "  \n",
      "          [[0.1280]],\n",
      "  \n",
      "          [[0.0000]]], grad_fn=<ReluBackward0>), padding=0, dilation=1, ceil_mode=False\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(3, 5, 5)\n",
    "print(nn.MaxPool2d(F.relu(nn.Conv2d(3, 6, 5)(input))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "tf = transforms.Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=tf,\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=tf,\n",
    "    # target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dl: DataLoader, model: MyCNN, loss: nn.CrossEntropyLoss, opt: torch.optim.SGD):\n",
    "    model.train()\n",
    "    for x, y in dl:\n",
    "        # x = x.to(device)\n",
    "        # y = y.to(device)\n",
    "        p = model(x)\n",
    "        # print(p)\n",
    "        # print(y)\n",
    "        l = loss(p, y)\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dl: DataLoader, model: MyCNN, loss: nn.CrossEntropyLoss):\n",
    "    model.eval()\n",
    "    tl, c = 0, 0\n",
    "    sz = len(dl.dataset)\n",
    "    b = len(dl)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dl:\n",
    "            # x = x.to(device)\n",
    "            # y = y.to(device)\n",
    "            p = model(x)\n",
    "            tl += loss(p, y).item()\n",
    "            c += (p.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    tl /= b\n",
    "    c /= sz\n",
    "    \n",
    "    print(tl)\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x17a672de0>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     train(train_dataloader, model, criterion, optimizer)\n\u001b[1;32m      4\u001b[0m     test(test_dataloader, model, criterion)\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dl, model, loss, opt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(dl: DataLoader, model: MyCNN, loss: nn\u001b[39m.\u001b[39mCrossEntropyLoss, opt: torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD):\n\u001b[1;32m      2\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m dl:\n\u001b[1;32m      4\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m         y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1035\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, fp)\n\u001b[1;32m     48\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x17a672de0>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i}\")\n",
    "    train(train_dataloader, model, criterion, optimizer)\n",
    "    test(test_dataloader, model, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
